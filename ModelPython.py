# -*- coding: utf-8 -*-
"""eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wd76X4kHhwZPb61wlYpk363mhf3MiLKs

#EDA
"""

#!/usr/bin/env python

#pip install kagglehub

import pandas as pd
import kagglehub
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, kurtosis

# Download latest version
path = kagglehub.dataset_download("stefanouccelli/tesis-1")

print("Path to dataset files:", path)

# List files in the downloaded directory
files_in_path = os.listdir(path)
print("Files in the downloaded directory:", files_in_path)

# Construct the full path to the downloaded file
old_file_path = os.path.join(path, files_in_path[0])

# Construct the new file path in a writable directory
new_file_path_in_writable = os.path.join( files_in_path[0])

# Copy the file to the writable directory
shutil.copyfile(old_file_path, new_file_path_in_writable)

print(f"Copied '{old_file_path}' to '{new_file_path_in_writable}'")

# Construct the final file path after removing '.crdownload'
final_file_path = new_file_path_in_writable.replace('.crdownload', '')

# Rename the copied file
os.rename(new_file_path_in_writable, final_file_path)

print(f"Renamed '{new_file_path_in_writable}' to '{final_file_path}'")

# Load the renamed file into a pandas DataFrame
df = pd.read_csv(final_file_path)

# Display the first few rows of the DataFrame
display(df.head())

df.info()

df.sample(3)

""" El archivo contiene un total de 3.706.070 registros y un total de  11 columnas
* step: Indica probablemente el momento (horas o secuencia) de la transacción.
* type: Tipo de transacción (CASH_OUT, PAYMENT, etc.).
* amount: Monto de la transacción.
* nameOrig, nameDest: Identificadores de origen y destino (anónimos).
* oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest: Saldos antes y después en origen y destino.
* isFraud: Indicador si la transacción es fraude.
* isFlaggedFraud: Indicador de alerta (¿posible fraude detectado automáticamente?).

"""

# Comprobar valores faltantes en cada columna
missing_summary = df.isnull().sum()
missing_summary_df = pd.DataFrame({"Columna": missing_summary.index, "Valores Faltantes": missing_summary.values})
missing_summary_df

"""imputacion de valores faltantes"""

#Eliminar las filas na
df = df[df['nameOrig'].notna()]

#Valores duplicados

duplicated_rows = df[df.duplicated(keep=False)]
num_duplicated_rows = duplicated_rows.shape[0]

num_duplicated_rows, duplicated_rows.head()

"""* No se encontraron valores duplicados y se elimino una fila que contenia valores nulos.
* Podemos proceder a realizar el analisis de los datos.

1.1 Analisis Univariado
"""

#Analisis Univariado

numericas = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']
categoricas = ['type', 'nameOrig', 'nameDest']

df[numericas].describe().T

# Histograma para cada variable numérica
for col in numericas:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], bins=50, kde=True)
    plt.title(f'Distribución de {col}')
    plt.xlabel(col)
    plt.ylabel('Frecuencia')
    plt.show()

display(df[df['amount']==0]['type'].count())
display(df[df['amount']>0]['type'].count())

df['type'].value_counts(normalize=True)

"""* Tenemos una distribución muy sesgada a la derecha con muchos valores bajos y muy pocos valores altos de todos los valores numericos sin contar los datos binarios.
* Tenemos outlier relacionados a los valores de las transacciones.
* En tipo de transacciones tenemos CASH_OUT con un 35%, PAYMENT con 33%, CASH-IN con 21%, TRANSFER 8% y DEBIT con 0.6%
"""

#Desbalanceo de las muestras'isFraud' y 'isFlaggedFraud'
df['isFraud'].value_counts(normalize=True)

"""* Podemos ver que el 99% de las transacciones no son fraudulentas y menos de un 0.1% son fraudulentas. por lo que tenemos un fuerte desbalanceo de las muestras.
* El desbalanceo de las muestras nos indican que hay una necedidad de oversamplig o undessamplig
* Las mejores meticas para la evaluación del modelo  son (F1, Recall y  Presición).
* La ingenieria de caracteristicas sera muy necesaria para poder identificar las transacciones fraudulentas.

Revisión de la asimetria y curtosis de los datos
"""

# Calcular asimetría (skewness) y curtosis
skewness = df[numericas].apply(skew)
kurt = df[numericas].apply(kurtosis)

# Mostrar los resultados
results = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurt})
results.round(3)

"""* La asimetria nos indica que hay muchos valores sesgados asia la derecha muchos valores bajos y otros extremadamente altos.
* La curtosisi nos indica valores outliers muy altos en especial para amount, fraude esta muy alto como lo hemos visto anteriormente por el desbalanceo de las muestras.
* Las transformaciones de los datos pueden ser un buen punto de partida, transformaciones logaritmicas, raiz cuadrada, yeo-jhonson u otras pueden ser importantes
"""

numericas

# Graficos de caja
plt.figure(figsize=(16, 8))
for i, col in enumerate(numericas, 1):
    plt.subplot(2, 4, i)
    sns.boxplot(x=df[col], color='deepskyblue')
    plt.title(f'Boxplot de {col}')

    plt.tight_layout()
plt.show()

"""2. Analisis Bivariado"""

# Definir variables numéricas relevantes
numeric_vars = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',
'oldbalanceDest', 'newbalanceDest', 'isFraud']

# 1. Heatmap para todo el dataset
corr_all = df[numeric_vars].corr()

# 2. Heatmap para isFraud == 1
corr_fraud = df[df['isFraud'] == 1][numeric_vars].corr()

# 3. Heatmap para isFraud == 0
corr_nofraud = df[df['isFraud'] == 0][numeric_vars].corr()

# Crear heatmaps
fig, axs = plt.subplots(1, 3, figsize=(21, 6))

sns.heatmap(corr_all, annot=True, cmap="coolwarm", ax=axs[0])
axs[0].set_title('Correlación - Todos')

sns.heatmap(corr_fraud, annot=True, cmap="coolwarm", ax=axs[1])
axs[1].set_title('Correlación - isFraud = 1')

sns.heatmap(corr_nofraud, annot=True, cmap="coolwarm", ax=axs[2])
axs[2].set_title('Correlación - isFraud = 0')

plt.tight_layout()
plt.show()
######

#Correlaciones
print("Corrleacionesfraude")
display(corr_fraud)
display(corr_nofraud)

import matplotlib.pyplot as plt
import seaborn as sns
# Plot de conteo diferenciado por fraude/no fraude
plt.figure(figsize=(10,6))
sns.countplot(data=df[df['isFraud']==1], x='type', hue='isFraud', palette='Set1')
plt.title("Distribución de transacciones por tipo y fraude")
plt.xlabel("Tipo de Transacción")
plt.ylabel("Cantidad de Transacciones")
plt.legend(title="¿Fraudulenta?", labels=["No", "Sí"])
plt.tight_layout()
plt.show()

# Filtrar solo las transacciones fraudulentas
fraud_df = df[df['isFraud'] == 1]

# Calcular el porcentaje de cada 'type' dentro de las transacciones fraudulentas
fraud_type_counts = fraud_df['type'].value_counts(normalize=True) * 100

# Mostrar el resultado en formato porcentaje
fraud_type_counts.round(2)

# Aplicar transformación logarítmica al monto (sumar 1 para evitar log(0))
df['log_amount'] = np.log1p(df['amount'])

# Crear el violin plot
plt.figure(figsize=(8, 5))
sns.violinplot(x='isFraud', y='log_amount', data=df, palette='Set2')

plt.title('Distribución logarítmica del monto ("amount") por clase de fraude')
plt.xlabel('isFraud (0 = No fraude, 1 = Fraude)')
plt.ylabel('log(1 + amount)')
plt.xticks([0,1], ['No fraude', 'Fraude'])
plt.show()

"""#### INGENIERIA DE DATOS MODELO ANALITICO PARA PREDECIR EL FRAUDE"""

df.columns

"""* Empezaremos por hacer una suposición del modelo,  queremos predecir la probabilidad que sea fraude una transacción, ya que de esta manera tendremos màs rango de maniobra en el modelo.

* las unicos tipos de variables con fraude son CASH_OUT (50.3%) Y TRANSFER (0.49%), utilizaremos estas variables en nuestro modelo.
"""

percentage = df[df['isFraud']==1]['type'].value_counts(normalize=True)
display(percentage)

#Almacenamos en un diccionario y realizamos un mpa con una nueva variable para modelar este porcentaje.
percentage['CASH_OUT']
percentage['TRANSFER']

dicc_percentage = {
    'CASH_OUT': percentage['CASH_OUT'],
    'TRANSFER': percentage['TRANSFER'],
    'PAYMENT': 0,
    'DEBIT': 0,
    'CASH_IN': 0
}

df['type_percentage'] = df['type'].map(dicc_percentage)
df[['type', 'type_percentage']].sample(5)

df['SQRT_ammount'] = np.sqrt(df.amount)

print('Ammount Normal')
display(df.pivot_table(index='type', columns='isFraud', values='amount', aggfunc='mean', fill_value=0))
print('Ammount Log')
display(df.pivot_table(index='type', columns='isFraud', values='log_amount', aggfunc='mean', fill_value=0))
print('Ammount Sqrt')
display(df.pivot_table(index='type', columns='isFraud', values='SQRT_ammount', aggfunc='mean', fill_value=0))

# Aplicar transformación logarítmica al monto (sumar 1 para evitar log(0))

# Crear el violin plot
plt.figure(figsize=(8, 5))
sns.violinplot(x='isFraud', y='SQRT_ammount', data=df, palette='Set2')

plt.title('Distribución Cuadratica del monto ("amount") por clase de fraude')
plt.xlabel('isFraud (0 = No fraude, 1 = Fraude)')
plt.ylabel('log(1 + amount)')
plt.xticks([0,1], ['No fraude', 'Fraude'])
plt.show()

"""Podemos identificar que el modelo con  SQRT es el que muestra mayor diferencia entre fraude y no fraude
1/0
CASH_OUT	387.376365	883.283541
TRANSFER	707.878268	892.652761
"""

# Identificación por rangos

origin_destinity = df[df['isFraud']==1].groupby(['type','nameOrig', 'nameDest']).agg({
    'isFraud':'count',
    'newbalanceDest':'mean',
    }
)

origin_destinity = origin_destinity.reset_index()
origin_destinity.sample(5)

origin_destinity[origin_destinity['type']=='TRANSFER']['newbalanceDest'].value_counts()

"""Los tipos de cuenta que transacciones TRANSFER que se vacian luego de latransacción tienen una probabilidad muy alta de ser una transacción fraudulenta"""

df['newbalanceDestSqrt'] = np.sqrt(df['newbalanceDest'])
df.pivot_table(index=['type'], columns='isFraud', values='newbalanceDestSqrt', aggfunc='mean', fill_value=0).reset_index()

"""crearemos una variable que nos indique si la cuenta fue vaciada"""

df['vacied_account'] = np.where( ( (df['type']=='TRANSFER') & (df['newbalanceDest']==0) ), 1, 0)

df.head()

"""Creamos una variable como diferencia porcentual entre el balance anterior y el nuevo balance

"""

df['balance_Change'] = (df['newbalanceDest'] - df['oldbalanceDest'])/(df['oldbalanceDest'])

epsilon = 1e-10
df['balance_Change_Origin'] = (df['newbalanceOrig'] - df['oldbalanceOrg'])/(df['oldbalanceOrg']+epsilon)
df['balance_Change_Origin'] = np.abs(df['balance_Change_Origin'])
df['balance_Change_Origin'] .sample(5)

# Asegura que balance_Change esté entre 0 y 100
df['balance_Change'] = df['balance_Change'].clip(0, 100)

bins = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, float('inf')]
labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]

df['balance_Change_Range'] = pd.cut(df['balance_Change'], bins=bins, labels=labels, include_lowest=True, right=False)

print(df['balance_Change_Range'].isnull().sum())

# Asegura que balance_Change esté entre 0 y 100

df['balance_Change_Origin'] = df['balance_Change_Origin'].clip(0, 100)

bins = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, float('inf')]
labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]

df['balance_Change_Origin_Range'] = pd.cut(df['balance_Change_Origin'], bins=bins, labels=labels, include_lowest=True, right=False)

print(df['balance_Change_Origin_Range'].isnull().sum())

df['balance_Change_Origin_Range'].sample(5)

"""Podemos ver que el cambio de"""

df.groupby('balance_Change_Origin_Range').aggregate({'isFraud':'value_counts'})

"""Los cambios porcentuales más grandes son en donde se presentan mayormente los fraudes, por lo que la variable balance_Change_Range mostrara un buen resultado en la predicción.

Modelo de Clasificación
"""

df.head()
df['balance_Change_Origin_Range'] = df['balance_Change_Origin_Range'].astype('int')
df['balance_Change_Range'] = df['balance_Change_Range'].astype('int')

df.groupby('type').aggregate({'isFraud':'value_counts'})

df.groupby('type').aggregate({'isFraud':'value_counts'})
df1 = df[df['type'].isin(['CASH_OUT', 'TRANSFER'])].copy()

#Seleccionar las caracteristicas para proyectar el modelo y la variable objetivo

x = df1[['type_percentage', 'log_amount', 'vacied_account', 'balance_Change_Range', 'newbalanceDestSqrt','balance_Change_Origin_Range']]
y = df1['isFraud']

#Crear la variable dummy para la variable categórica 'type'
#x = pd.get_dummies(x, columns=['type'], drop_first=True)
x.head()

x.info()

#Dividir el conjunto en entrenamiento y prueba

x['balance_Change_Range'].unique()
x['balance_Change_Range'] = x['balance_Change_Range'].astype(int)

####
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123, stratify=y)

#Generar un de regresion logistica
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=10000)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc, f1_score, recall_score

print(f'recall_score: {recall_score(y_test, y_pred)}')
print("f1-Score:", f1_score(y_test, y_pred))

#

#Graficar la curva ROC
y_prob = model.predict_proba(x_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'Curva ROC (auc = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.title('Curva ROC')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.legend(loc='lower right')
plt.show()

#Graficar la matriz de confusion

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Realidad')
plt.show()

pip install xgboost

#utilizar Xgboost para mejorar el modelo
from xgboost import XGBClassifier


xgb_model = XGBClassifier(use_label_encoder=False,
                           eval_metric='pre',
                            random_state=123,
                            n_jobs=-1,
                            max_depth=5,
                            max_features='log2',
                            learning_rate=0.1,
                            n_estimators=100,
                            sample_weight = 1,
                            subsample = 0.8,
                            #scale_pos_weight= (y_train==1).sum() / (y_train==0).sum()
                            )
xgb_model.fit(x_train, y_train)


y_proba = xgb_model.predict_proba(x_test)[:, 1]
y_pred_thresh = (y_proba > 0.5).astype(int)  # umbral ajustado
recall_thresh = recall_score(y_test, y_pred_thresh)
print("Recall con umbral:", recall_thresh)
print("f1-Score con umbral:", f1_score(y_test, y_pred_thresh))

#Graficar la matriz de confusion

cm = confusion_matrix(y_test, y_pred_thresh)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Realidad')
plt.show()

#Random Forest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=3,
                                  random_state=42,
                                  n_jobs=-1,
                                  criterion = 'log_loss',
                                  max_depth=5,
                                  max_features='log2',
                                  #class_weight='balanced'
                                  )
rf_model.fit(x_train, y_train)
y_pred = rf_model.predict(x_test)
recall = recall_score(y_test, y_pred)
print(i)
print("Recall con Random Forest:", recall)
print("f1-Score con Random Forest:", f1_score(y_test, y_pred))

#Matriz de confusion

#Graficar la matriz de confusion

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Realidad')
plt.show()

from sklearn.ensemble import VotingClassifier

ensemble_model = VotingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('rf', rf_model)
    ],
    voting='soft'  # usa probabilidades para mayor sensibilidad
)
ensemble_model.fit(x_train, y_train)
y_pred_ens = ensemble_model.predict(x_test)
recall_ens = recall_score(y_test, y_pred_ens)
print("Recall con Ensemble:", recall_ens)
print(f'f1_socore con Ensemble:', f1_score(y_test, y_pred_ens))

from sklearn.ensemble import ExtraTreesClassifier
for i in range(1,10,1):
    et_model = ExtraTreesClassifier(n_estimators=i,
                                    random_state=42,
                                    n_jobs=-1,
                                    criterion = 'log_loss',
                                    max_depth=5,
                                    max_features='log2',
                                    class_weight='balanced'
                                    )
    et_model.fit(x_train, y_train)
    y_pred_et = et_model.predict(x_test)
    recall_et = recall_score(y_test, y_pred_et)
    print(i)
    print("Recall con Extra Trees:", recall_et)
    print(f'f1-score: {f1_score(y_test, y_pred_et)}')

et_model = ExtraTreesClassifier(n_estimators=8,
                                    random_state=42,
                                    n_jobs=-1,
                                    criterion = 'log_loss',
                                    max_depth=5,
                                    max_features='log2',
                                    class_weight='balanced')
et_model.fit(x_train, y_train)
y_pred_et = et_model.predict(x_test)
recall_et = recall_score(y_test, y_pred_et)
print(i)
print("Recall con Extra Trees:", recall_et)
print("f1-Score con Extra Trees:", f1_score(y_test, y_pred_et))

#Graficar la matriz de confusion

cm = confusion_matrix(y_test, y_pred_et)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Realidad')
plt.show()